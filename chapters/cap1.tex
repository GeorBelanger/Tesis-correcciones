\noindent
El intento por poder entender, analizar y, más importante, reproducir la inteligencia, no es una búsqueda que se pueda atribuir únicamente a los últimos años. Durante siglos, los humanos han soñado con crear máquinas que piensen. Con la llegada de las computadoras programables, ese sueño ha tomado grandes pasos. Hoy en día, la inteligencia artificial es una disciplina con un gran rango de aplicaciones y temas de investigación activos. Por ejemplo, se utiliza software inteligente para manejar coches automáticamente, predecir el precio de acciones en el futuro o hacer diagnósticos en medicina. \cite{goodfellow-et-al-2016}  \cite{hastie01statisticallearning}
\cite{Haugeland:1985:AIV:4694}

\vspace{1em}

En sus comienzos, la inteligencia artificial pudo resolver problemas intelectualemente complicados para los humanos, pero relativamente fáciles para las computadoras: problemas que se pueden describir con una lista de reglas formales o matemáticas. El verdadero reto de la inteligencia artificial resultó ser resolver tareas que son fáciles para las personas de realizar, pero difíciles para las personas de describir formalmente, problemas que resolvemos intuitivamente, que sentimos automáticos, como comprender palabras habladas o reconocer caras de personas en imágenes. 
\cite{goodfellow-et-al-2016}

\vspace{1em}

Una solución que se ha encontrado para resolver estos problemas es permitir a las computadoras aprender de la experiencia y entender al mundo en términos de una jerarquía de conceptos, con cada concepto siendo definido en términos de su relación con conceptos más sencillos. Si dibujamos una gráfica mostrando cómo estos conceptos se construyen uno encima del otro, el gráfico es profundo con muchas capas. Por esta razón, este enfoque de inteligencia artificial se conoce con el nombre de aprendizaje profundo. 
\cite{deep-learning-methods-and-applications}
\cite{goodfellow-et-al-2016}

\vspace{1em}

En este trabajo, abordaremos el aprendizaje profundo desde un punto de vista teórico y práctico. La parte teórica se basará en el libro "Deep Learning" de Ian Goodfellow, Yoshua Bengio y Aaron Courville. En específico, el capítulo 2 explicará los conceptos básicos de aprendizaje de máquina y el capítulo 3 describirá el aprendizaje profundo: redes neuronales, redes neuronales recurrentes y, finalemente, el modelo que se utilizará en la parte práctica, redes neuronales recurrentes con "Long Short-Term Memory".

\vspace{1em}

Los capítulos 4 y 5 se basarán en los experimentos realizados por Alex Graves, investigador de Google Deepmind, en su artículo "Generating Sequences With Recurrent Neural Networks", publicado en 2014. En el capítulo 4 se generarán sucesiones de texto (donde los datos son discretos)  y en el capítulo 5 se generarán sucesiones de caligrafía en línea (donde los datos son reales). Por último, en el capítulo 6 se harán conclusiones sobre los experimentos y describirán algunas nuevas posibilidades de investigación. \cite{DBLP:journals/corr/Graves13}



